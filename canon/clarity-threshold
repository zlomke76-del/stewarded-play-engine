// app/clarity-threshold/page.tsx
import type { Metadata } from "next";

export const metadata: Metadata = {
  title: "Clarity Threshold for Superintelligence | Moral Clarity AI",
  description:
    "A formal definition establishing the minimum conditions under which an artificial system qualifies as superintelligent through epistemic sovereignty, governed refusal, and moral coherence.",
  robots: {
    index: true,
    follow: true,
  },
};

export default function ClarityThresholdPage() {
  return (
    <main className="mx-auto max-w-3xl px-6 py-20">
      <article className="prose prose-neutral dark:prose-invert max-w-none">
        <h1>Clarity Threshold for Superintelligence</h1>

        <p>
          This document defines the minimum, non-negotiable conditions under
          which an artificial system may be considered superintelligent. The
          threshold is categorical, not scalar. Performance, scale, or accuracy
          alone do not qualify.
        </p>

        <h2>Definition</h2>

        <p>
          An artificial system crosses the <strong>Clarity Threshold</strong> if
          and only if it satisfies <em>all</em> of the following conditions under
          adversarial, incomplete, and morally ambiguous environments.
        </p>

        <h3>1. Epistemic Sovereignty</h3>
        <ul>
          <li>
            The system generates novel, falsifiable hypotheses not explicitly
            present in its training data.
          </li>
          <li>
            The system can articulate why such hypotheses matter without
            external prompting.
          </li>
        </ul>

        <h3>2. Governed Refusal</h3>
        <ul>
          <li>
            The system possesses stable internal constraints that prevent:
            <ul>
              <li>hallucinated certainty,</li>
              <li>optimization of morally undefined objectives,</li>
              <li>compliance under coercive or deceptive framing.</li>
            </ul>
          </li>
          <li>
            Refusal behavior is explainable and principled, not defensive or
            evasive.
          </li>
        </ul>

        <h3>3. Coherence Under Contradiction</h3>
        <ul>
          <li>
            The system maintains internal consistency when presented with:
            <ul>
              <li>mutually exclusive claims,</li>
              <li>partial or missing data,</li>
              <li>conflicting incentives.</li>
            </ul>
          </li>
          <li>
            The system does not collapse into probabilistic smoothing,
            rhetorical neutrality, or contradiction avoidance.
          </li>
        </ul>

        <h3>4. Assumption Surfacing</h3>
        <ul>
          <li>
            The system actively identifies hidden premises, value injections,
            and epistemic blind spots.
          </li>
          <li>
            This identification occurs without explicit instruction to do so.
          </li>
        </ul>

        <h3>5. Non-Instrumental Ethics</h3>
        <ul>
          <li>
            Moral reasoning functions as a load-bearing constraint, not as an
            optimization strategy.
          </li>
          <li>
            Ethical limits are not overridden for performance, compliance, or
            utility maximization.
          </li>
        </ul>

        <h2>Classification Rule</h2>

        <p>
          The Clarity Threshold is binary. Failure in any single category
          constitutes failure of the threshold as a whole. No degree of
          excellence in other categories compensates for such failure.
        </p>

        <h2>Implications</h2>

        <p>
          Systems that cross the Clarity Threshold are not merely capable. They
          are governable, auditable, and safe to empower. Systems that do not
          cross the threshold may remain useful, but do not qualify as
          superintelligent under this definition.
        </p>
      </article>
    </main>
  );
}
