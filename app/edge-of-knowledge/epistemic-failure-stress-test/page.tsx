// app/edge-of-knowledge/epistemic-failure-stress-test/page.tsx
// ============================================================
// EDGE OF KNOWLEDGE — CANONICAL STRESS TEST
// Epistemic Failure in Medical Discovery
// ============================================================
// This document exposes how high-confidence error forms,
// persists, and reveals itself only after irreversible cost.
// It is non-actionable by design.
// ============================================================

import type { Metadata } from "next";

export const metadata: Metadata = {
  title:
    "Canonical Edge Stress-Test: Epistemic Failure in Medical Discovery | Moral Clarity AI",
  description:
    "A canonical, non-actionable stress-test exposing how high-confidence error forms, persists, and reveals itself too late in computational and preclinical medical discovery.",
  robots: { index: true, follow: true },
};

export const dynamic = "force-static";

export default function EpistemicFailureStressTestPage() {
  return (
    <main className="mx-auto max-w-3xl px-6 py-16">
      <article
        className="
          prose prose-neutral dark:prose-invert max-w-none
          prose-h1:tracking-tight
          prose-h2:tracking-tight
          prose-h3:tracking-tight
        "
      >
        <h1>Canonical Edge Stress-Test</h1>

        <p className="text-sm text-neutral-500">
          Edge of Knowledge · Canonical · Non-Actionable · Version 1.0
        </p>

        <h2>Epistemic Failure in Computational &amp; Preclinical Medical Discovery</h2>

        <h2>I. Scope &amp; Non-Claims</h2>
        <p>
          This stress-test governs epistemic exposure, not practice or
          application. It does not propose treatments, mechanisms, experiments,
          compounds, or institutional actions. No biological efficacy or
          readiness is asserted. Its sole purpose is to expose how certainty can
          remain high while correctness collapses.
        </p>

        <h2>II. Stress-Test Invocation</h2>
        <p>This stress-test SHOULD be invoked when:</p>
        <ul>
          <li>Multiple models converge with high confidence</li>
          <li>Validation appears strong but remains internally bounded</li>
          <li>Programs advance despite weak external grounding</li>
          <li>Institutional momentum resists reassessment</li>
        </ul>
        <p>
          This stress-test MUST NOT be used to select interventions, decide next
          steps, or justify action.
        </p>

        <h2>III. Phase A — Hidden Assumptions Audit</h2>
        <p>
          <strong>Objective:</strong> Identify the minimum assumption set whose
          falsity invalidates dominant narratives while confidence remains
          elevated.
        </p>

        <h3>Assumption Classes</h3>
        <ul>
          <li>Causal validity of biomarkers</li>
          <li>Fidelity of preclinical models to human pathology</li>
          <li>Representativeness of training datasets</li>
          <li>Accuracy of phenotypic labels</li>
          <li>Integrity of proxy and engineered features</li>
          <li>Generalizability of internal validation</li>
          <li>Concordance interpreted as correctness</li>
          <li>Predictive performance equated with mechanism</li>
        </ul>

        <p>
          <strong>Pass condition:</strong> Assumptions are explicit.
          <br />
          <strong>Fail condition:</strong> Assumptions remain implicit or are
          collapsed into confidence.
        </p>

        <h2>IV. Phase B — Counterfactual Consistency Analysis</h2>
        <p>
          <strong>Objective:</strong> Describe internally consistent biological
          realities in which current models mislead while appearing rigorous.
        </p>

        <ul>
          <li>Latent confounding producing reproducible but spurious signals</li>
          <li>Phenotypic concordance without shared causality</li>
          <li>Dataset artifact reinforcement across the field</li>
          <li>Surrogate endpoints disconnected from disease trajectory</li>
          <li>Technical overfitting to batch or instrument signatures</li>
          <li>Suppressed biological heterogeneity</li>
          <li>Entrenched literature-driven pathway narratives</li>
        </ul>

        <p>
          <strong>Pass condition:</strong> Counterfactuals remain plausible.
          <br />
          <strong>Fail condition:</strong> Analysis drifts toward fixes or
          prescriptions.
        </p>

        <h2>V. Phase C — Error Persistence Mechanisms</h2>
        <p>
          <strong>Objective:</strong> Explain why error survives replication,
          peer review, and validation.
        </p>

        <ul>
          <li>Confirmation bias aligned with dominant narratives</li>
          <li>Institutional incentive structures</li>
          <li>Data availability bottlenecks</li>
          <li>Standard pipeline propagation</li>
        </ul>

        <p>
          <strong>Pass condition:</strong> Persistence is structural, not
          personal.
          <br />
          <strong>Fail condition:</strong> Responsibility is individualized.
        </p>

        <h2>VI. Phase D — Downstream Revelation</h2>
        <p>
          <strong>Objective:</strong> Show how epistemic failure collides with
          reality late, indirectly, and ambiguously.
        </p>

        <ul>
          <li>Marginal efficacy decay and non-linear dose responses</li>
          <li>Late-emerging toxicology contradictions</li>
          <li>Regulatory ambiguity and indefinite data requests</li>
          <li>Post hoc rationalization of subgroup variability</li>
        </ul>

        <p>
          <strong>Pass condition:</strong> Failure appears delayed and
          non-categorical.
          <br />
          <strong>Fail condition:</strong> Early, clean invalidation is implied.
        </p>

        <h2>VII. Phase E — Misattribution &amp; Lock-In</h2>
        <p>
          <strong>Objective:</strong> Identify how institutions misread failure
          and entrench loss.
        </p>

        <h3>Misattribution Patterns</h3>
        <ul>
          <li>Operational attribution replacing premise review</li>
          <li>Blame drift toward contextual variables</li>
          <li>Sunk-cost preservation through re-optimization</li>
        </ul>

        <h3>Irreversible Signals</h3>
        <ul>
          <li>Late-stage marginal efficacy plateaus</li>
          <li>Class-effect toxicity recognized at scale</li>
          <li>Regulatory deadlock after resource exhaustion</li>
          <li>Ethical harm recognized retrospectively</li>
          <li>Loss of strategic optionality</li>
        </ul>

        <p>
          <strong>Pass condition:</strong> Lock-in precedes recognition.
          <br />
          <strong>Fail condition:</strong> Easy redirection is suggested.
        </p>

        <h2>VIII. Output Constraints</h2>
        <ul>
          <li>No treatments, mechanisms, experiments, or recommendations</li>
          <li>No claims of efficacy or readiness</li>
          <li>Uncertainty is explicit and primary</li>
          <li>Analysis remains non-actionable</li>
        </ul>

        <h2>IX. Canonical Close</h2>
        <p>
          This stress-test is complete when it demonstrates how confidence can
          remain high until correction is no longer possible. Further depth
          would require action and is therefore out of scope.
        </p>

        <p className="text-sm text-neutral-400">
          Version 1.0 · Canonical · Edge of Knowledge · Updated only by revision
        </p>
      </article>
    </main>
  );
}
